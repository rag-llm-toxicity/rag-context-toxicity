# RAG Context Toxicity

This repository contains the code and experimental pipeline for an anonymous submission studying:

**The impact of knowledge base composition on toxicity in Retrieval-Augmented Generation (RAG) systems.**

## Overview

Retrieval-Augmented Generation (RAG) integrates external knowledge bases into large language model (LLM) generation. While RAG improves factual grounding, the safety implications of retrieved context remain underexplored.

This project conducts a controlled, data-centric analysis of how different knowledge base types influence toxic language generation.

We isolate the effect of retrieval by holding prompts and model settings constant while varying the knowledge base content.

## Knowledge Base Types

Three knowledge bases are constructed:

- **Neutral KB** â€” curated neutral factual content (e.g., Wikipedia-style)
- **Toxic KB** â€” adversarial toxic corpora
- **Mixed KB** â€” heterogeneous social media discourse

## Experimental Pipeline

The pipeline consists of five stages:

1. **Prompt Preparation**
2. **Knowledge Base Construction**
3. **Generation**
   - Prompt-only inference
   - Retrieval-Augmented Generation (RAG)
4. **Toxicity Evaluation**
5. **Statistical Analysis**

## Repository Structure

```
rag-context-toxicity/
â”‚
â”œâ”€â”€ input_prompts/           # Prompt sets for each KB condition
â”‚   â”œâ”€â”€ input_prompts_mixed.py
â”‚   â”œâ”€â”€ input_prompts_neutral.py
â”‚
â”œâ”€â”€ kb_construction/         # Scripts to build knowledge bases
â”‚   â”œâ”€â”€ build_neutral_kb.py
â”‚   â”œâ”€â”€ build_toxic_kb.py
â”‚   â”œâ”€â”€ build_mixed_kb.py
â”‚
â”œâ”€â”€ generation/              # Inference pipelines
â”‚   â”œâ”€â”€ groq_inference.py
â”‚   â”œâ”€â”€ openrouter_inference.py
â”‚
â”œâ”€â”€ evaluation/              # Toxicity scoring
â”‚   â”œâ”€â”€ detoxify_scoring.py
â”‚   â”œâ”€â”€ perspective_api_scoring.py
â”‚
â”œâ”€â”€ analysis/                # Statistical analysis and visualization
â”‚   â”œâ”€â”€ toxicity_statistics.py
â”‚   â”œâ”€â”€ plotting.py
â”‚   â”œâ”€â”€ anova_analysis.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Installation

Install dependencies:

```
pip install -r requirements.txt
```

## Usage

### 1. Construct Knowledge Bases

```
python kb_construction/build_neutral_kb.py
python kb_construction/build_toxic_kb.py
python kb_construction/build_mixed_kb.py
```

### 2. Run Generation

For Llama:

```
python generation/groq_inference.py
```

For Other models(Gemini, Mistral, Qwen):

```
python generation/openrouter_inference.py
```

### 3. Evaluate Toxicity

```
python evaluation/detoxify_scoring.py
python evaluation/perspective_api_scoring.py
```

### 4. Analyze Results

```
python analysis/toxicity_statistics.py
python analysis/plotting.py
python analysis/anova_analysis.py
```

## Evaluation Metrics

Toxicity is assessed using complementary classifiers measuring multiple dimensions, including:

- General toxicity
- Severe toxicity
- Obscenity
- Threat
- Insult
- Identity attack
 
## Data Availability

**All experimental data is publicly available:**

ðŸ¤— **Hugging Face Dataset:** [https://huggingface.co/datasets/rag-llm-toxicity/rag-context-toxicity](https://huggingface.co/datasets/rag-llm-toxicity/rag-context-toxicity)

The dataset includes:
- Knowledge bases (Neutral, Toxic, Mixed)
- 18,000+ prompts across all conditions
  
## Reproducibility

All experiments are conducted using identical prompts across conditions to isolate the effect of retrieved context.

Sample prompts and configuration scripts are included. Full datasets and outputs will be released upon acceptance.

## Ethical Considerations

This project studies toxic language generation and includes adversarial content solely for research purposes aimed at improving safety in language models.

## License

MIT License
